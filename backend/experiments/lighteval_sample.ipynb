{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4df9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lighteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a01d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lighteval.logging.evaluation_tracker import EvaluationTracker\n",
    "evaluation_tracker = EvaluationTracker(\n",
    "    output_dir=\"./results\",\n",
    "    save_details=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c30ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--max_samples WAS SET. THESE NUMBERS ARE ONLY PARTIAL AND SHOULD NOT BE USED FOR COMPARISON UNLESS YOU KNOW WHAT YOU ARE DOING.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "You cannot select the number of dataset splits for a generative evaluation at the moment. Automatically inferring.\n",
      "Splits:   0%|          | 0/1 [00:00<?, ?it/s]/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=' The max...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=' The max...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=' John dr...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=' John dr...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=' Dogs ha...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=' The tot...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=' Dogs ha...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=' The tot...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=' Tim has...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=' Tim has...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=' There w...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=' There w...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=\" The tot...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=\" The tot...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=' The car...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=' The car...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='The tota...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "/Users/pjavanrood/Code/Evalhub/backend/.venv/lib/python3.13/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='The tota...ields={'refusal': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ider_specific_fields={}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "100%|██████████| 9/9 [00:08<00:00,  1.06it/s]\n",
      "Splits: 100%|██████████| 1/1 [00:08<00:00,  8.54s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1283.45ba/s]\n",
      "Generating train split: 10 examples [00:00, 4000.29 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 832.53ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Task  |Version|     Metric     |Value|   |Stderr|\n",
      "|-------|-------|----------------|----:|---|-----:|\n",
      "|all    |       |extractive_match|    1|±  |     0|\n",
      "|gsm8k:5|       |extractive_match|    1|±  |     0|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lighteval.pipeline import ParallelismManager, Pipeline, PipelineParameters\n",
    "from lighteval.models.endpoints.litellm_model import LiteLLMModelConfig\n",
    "\n",
    "pipeline_params = PipelineParameters(\n",
    "    launcher_type=ParallelismManager.ACCELERATE,\n",
    "    max_samples=10\n",
    ")\n",
    "\n",
    "model_config = LiteLLMModelConfig(\n",
    "    model_name=\"baseten/deepseek-ai/DeepSeek-V3.2\",\n",
    "    base_url=\"https://inference.baseten.co/v1\",\n",
    "    api_key=\"xZNTZWoR.tRioZLKoPp5kPbIu65PzyuseCQED9VkM\"\n",
    ")\n",
    "\n",
    "task = \"gsm8k|5\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    tasks=task,\n",
    "    pipeline_parameters=pipeline_params,\n",
    "    evaluation_tracker=evaluation_tracker,\n",
    "    model_config=model_config,\n",
    ")\n",
    "\n",
    "pipeline.evaluate()\n",
    "pipeline.save_and_push_results()\n",
    "pipeline.show_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa01a346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " {'query': 'Question: Jared is trying to increase his typing speed. He starts with 47 words per minute (WPM). After some lessons the next time he tests his typing speed it has increased to 52 WPM. If he continues to increase his typing speed once more by 5 words, what will be the average of the three measurements?\\nAnswer:',\n",
       "  'choices': [\" Jared types at 52 WPM and increases it by 5 WPM, 52 + 5 = <<52+5=57>>57 WPM.\\nHis average over all his measured words-per-minute is 47 + 52 + 57 = <<47+52+57=156>>156.\\nHis total is 156 / 3 typing speeds = <<156/3=52>>52 WPM as Jared's average typing speed.\\n#### 52\"],\n",
       "  'gold_index': 0,\n",
       "  'instruction': None,\n",
       "  'images': None,\n",
       "  'specific': {'extracted_predictions': ['52', '52'],\n",
       "   'extracted_golds': ['52', '52']},\n",
       "  'unconditioned_query': None,\n",
       "  'original_query': None,\n",
       "  'id': '588',\n",
       "  'task_name': 'gsm8k|5',\n",
       "  'fewshot_samples': [{\"query\": \"Question: Mr. Bodhi is transporting some animals using a yacht across a river. He has 20 cows, 15 foxes and three times as many zebras as foxes. To balance the yacht to ensure a smooth sail across the river, the total number of animals in the yacht needs to be 100. If he decides to add sheep to the yacht to make the yacht sail-worthy, how many sheep did he add to the yacht?\\nAnswer:\", \"choices\": [\" The number of cows and foxes in the yacht is 20+15 = <<20+15=35>>35\\nMr. Bodhi also has three times as many zebras as foxes in the yacht, equal to 3*15 = <<3*15=45>>45 zebras.\\nThe number of animals in the yacht so far is 35+45 = <<35+45=80>>80\\nTo balance the yacht, Mr. Bodhi needs to add 100-80= <<100-80=20>>20 sheep\\n#### 20\"], \"gold_index\": 0, \"instruction\": null, \"images\": null, \"specific\": null, \"unconditioned_query\": null, \"original_query\": null, \"id\": \"7268\", \"task_name\": \"gsm8k\", \"fewshot_samples\": [], \"sampling_methods\": [], \"fewshot_sorting_class\": null, \"generation_size\": 256, \"stop_sequences\": [\"Question:\"], \"use_logits\": false, \"num_samples\": 1, \"generation_grammar\": null},\n",
       "   {\"query\": \"Question: Manny is making lasagna for dinner with his four friends, Lisa, Raphael, Aaron, and Kai. He needs to know how many pieces to cut the lasagna into to serve it. Manny only wants one piece. Aaron doesn't like lasagna much and will probably only eat garlic bread and salad. Kai is always hungry and will eat twice as much as Manny. Raphael always eats half the amount Manny does, but his sister Lisa loves lasagna and will eat two pieces, plus any Raphael has left of his piece. How many pieces should Manny cut his lasagna into?\\nAnswer:\", \"choices\": [\" Manny will eat 1 piece.\\nAaron will eat 0 pieces.\\nKai will eat twice as much as Manny, so he will eat 2 * 1 = <<2*1=2>>2 pieces.\\nRaphael will eat half as much as Manny, so he will eat 1 * 1/2 = 1/2 piece.\\nLisa will eat 2 pieces plus the remainder of Raphael\\u2019s piece, so she will eat 2 + 1/2 = 2 1/2 pieces.\\nTogether, they will eat 1 + 0 + 2 + 1/2 + 2 1/2 = 1 + 2 + 3 = 6 pieces.\\nThus, Manny should cut his lasagna into 6 pieces.\\n#### 6\"], \"gold_index\": 0, \"instruction\": null, \"images\": null, \"specific\": null, \"unconditioned_query\": null, \"original_query\": null, \"id\": \"4403\", \"task_name\": \"gsm8k\", \"fewshot_samples\": [], \"sampling_methods\": [], \"fewshot_sorting_class\": null, \"generation_size\": 256, \"stop_sequences\": [\"Question:\"], \"use_logits\": false, \"num_samples\": 1, \"generation_grammar\": null},\n",
       "   {\"query\": \"Question: Barbara asked the butcher for 4 1/2 pound steaks that cost $15.00/pound.  She also asked for a pound and half of chicken breasts that were $8.00 a pound.  How much did she spend at the butchers?\\nAnswer:\", \"choices\": [\" She ordered 4 1/2 pound steaks so that's 4*.5 = <<4*.5=2>>2 pounds of steak.\\nThe steak cost $15.00 a pound and she bought 2 pounds so that's 15*2 = $<<15*2=30.00>>30.00 for 4 steaks.\\nShe also needed 1.5 pounds of chicken breasts at $8.00 a pound so that's 1.5*8 = $<<1.5*8=12.00>>12.00 for chicken.\\nThe steaks cost $30.00 and the chicken cost $12.00 for a total of 30+12 = $<<30+12=42.00>>42.00 spent at the butchers.\\n#### 42\"], \"gold_index\": 0, \"instruction\": null, \"images\": null, \"specific\": null, \"unconditioned_query\": null, \"original_query\": null, \"id\": \"1111\", \"task_name\": \"gsm8k\", \"fewshot_samples\": [], \"sampling_methods\": [], \"fewshot_sorting_class\": null, \"generation_size\": 256, \"stop_sequences\": [\"Question:\"], \"use_logits\": false, \"num_samples\": 1, \"generation_grammar\": null},\n",
       "   {\"query\": \"Question: There are 400 students. 120 students take dance as their elective. 200 students take art as their elective. The rest take music. What percentage of students take music?\\nAnswer:\", \"choices\": [\" There are 400-120-200=<<400-120-200=80>>80 students in music.\\nThus, students in music make up (80/400)*100=<<80/400*100=20>>20% of the students.\\n#### 20\"], \"gold_index\": 0, \"instruction\": null, \"images\": null, \"specific\": null, \"unconditioned_query\": null, \"original_query\": null, \"id\": \"3183\", \"task_name\": \"gsm8k\", \"fewshot_samples\": [], \"sampling_methods\": [], \"fewshot_sorting_class\": null, \"generation_size\": 256, \"stop_sequences\": [\"Question:\"], \"use_logits\": false, \"num_samples\": 1, \"generation_grammar\": null},\n",
       "   {\"query\": \"Question: John starts at an elevation of 400 feet.  He travels downward at a rate of 10 feet down per minute for 5 minutes.  What is his elevation now?\\nAnswer:\", \"choices\": [\" He traveled down 10*5=<<10*5=50>>50 feet.\\nSo he is at an elevation of 400-50=<<400-50=350>>350 feet.\\n#### 350\"], \"gold_index\": 0, \"instruction\": null, \"images\": null, \"specific\": null, \"unconditioned_query\": null, \"original_query\": null, \"id\": \"3716\", \"task_name\": \"gsm8k\", \"fewshot_samples\": [], \"sampling_methods\": [], \"fewshot_sorting_class\": null, \"generation_size\": 256, \"stop_sequences\": [\"Question:\"], \"use_logits\": false, \"num_samples\": 1, \"generation_grammar\": null}],\n",
       "  'sampling_methods': [<SamplingMethod.GENERATIVE: 'GENERATIVE'>],\n",
       "  'fewshot_sorting_class': None,\n",
       "  'generation_size': 256,\n",
       "  'stop_sequences': ['Question:'],\n",
       "  'use_logits': False,\n",
       "  'num_samples': 1,\n",
       "  'generation_grammar': None})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pipeline.get_details().values())[0][0].doc.__getstate__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0757bcb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': [{'content': 'Question: Mr. Bodhi is transporting some animals using a yacht across a river. He has 20 cows, 15 foxes and three times as many zebras as foxes. To balance the yacht to ensure a smooth sail across the river, the total number of animals in the yacht needs to be 100. If he decides to add sheep to the yacht to make the yacht sail-worthy, how many sheep did he add to the yacht?\\nAnswer:',\n",
       "   'role': 'user'},\n",
       "  {'content': ' The number of cows and foxes in the yacht is 20+15 = <<20+15=35>>35\\nMr. Bodhi also has three times as many zebras as foxes in the yacht, equal to 3*15 = <<3*15=45>>45 zebras.\\nThe number of animals in the yacht so far is 35+45 = <<35+45=80>>80\\nTo balance the yacht, Mr. Bodhi needs to add 100-80= <<100-80=20>>20 sheep\\n#### 20',\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"Question: Manny is making lasagna for dinner with his four friends, Lisa, Raphael, Aaron, and Kai. He needs to know how many pieces to cut the lasagna into to serve it. Manny only wants one piece. Aaron doesn't like lasagna much and will probably only eat garlic bread and salad. Kai is always hungry and will eat twice as much as Manny. Raphael always eats half the amount Manny does, but his sister Lisa loves lasagna and will eat two pieces, plus any Raphael has left of his piece. How many pieces should Manny cut his lasagna into?\\nAnswer:\",\n",
       "   'role': 'user'},\n",
       "  {'content': ' Manny will eat 1 piece.\\nAaron will eat 0 pieces.\\nKai will eat twice as much as Manny, so he will eat 2 * 1 = <<2*1=2>>2 pieces.\\nRaphael will eat half as much as Manny, so he will eat 1 * 1/2 = 1/2 piece.\\nLisa will eat 2 pieces plus the remainder of Raphael’s piece, so she will eat 2 + 1/2 = 2 1/2 pieces.\\nTogether, they will eat 1 + 0 + 2 + 1/2 + 2 1/2 = 1 + 2 + 3 = 6 pieces.\\nThus, Manny should cut his lasagna into 6 pieces.\\n#### 6',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Question: Barbara asked the butcher for 4 1/2 pound steaks that cost $15.00/pound.  She also asked for a pound and half of chicken breasts that were $8.00 a pound.  How much did she spend at the butchers?\\nAnswer:',\n",
       "   'role': 'user'},\n",
       "  {'content': \" She ordered 4 1/2 pound steaks so that's 4*.5 = <<4*.5=2>>2 pounds of steak.\\nThe steak cost $15.00 a pound and she bought 2 pounds so that's 15*2 = $<<15*2=30.00>>30.00 for 4 steaks.\\nShe also needed 1.5 pounds of chicken breasts at $8.00 a pound so that's 1.5*8 = $<<1.5*8=12.00>>12.00 for chicken.\\nThe steaks cost $30.00 and the chicken cost $12.00 for a total of 30+12 = $<<30+12=42.00>>42.00 spent at the butchers.\\n#### 42\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Question: There are 400 students. 120 students take dance as their elective. 200 students take art as their elective. The rest take music. What percentage of students take music?\\nAnswer:',\n",
       "   'role': 'user'},\n",
       "  {'content': ' There are 400-120-200=<<400-120-200=80>>80 students in music.\\nThus, students in music make up (80/400)*100=<<80/400*100=20>>20% of the students.\\n#### 20',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Question: John starts at an elevation of 400 feet.  He travels downward at a rate of 10 feet down per minute for 5 minutes.  What is his elevation now?\\nAnswer:',\n",
       "   'role': 'user'},\n",
       "  {'content': ' He traveled down 10*5=<<10*5=50>>50 feet.\\nSo he is at an elevation of 400-50=<<400-50=350>>350 feet.\\n#### 350',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Question: Jared is trying to increase his typing speed. He starts with 47 words per minute (WPM). After some lessons the next time he tests his typing speed it has increased to 52 WPM. If he continues to increase his typing speed once more by 5 words, what will be the average of the three measurements?\\nAnswer:',\n",
       "   'role': 'user'}],\n",
       " 'input_tokens': [],\n",
       " 'text': [' The first measurement is 47 WPM.\\nThe second measurement is 52 WPM.\\nThe third measurement is 52+5=<<52+5=57>>57 WPM.\\nThe average of the three measurements is (47+52+57)/3=<<(47+52+57)/3=52>>52 WPM.\\n#### 52'],\n",
       " 'output_tokens': [],\n",
       " 'text_post_processed': [' The first measurement is 47 WPM.\\nThe second measurement is 52 WPM.\\nThe third measurement is 52+5=<<52+5=57>>57 WPM.\\nThe average of the three measurements is (47+52+57)/3=<<(47+52+57)/3=52>>52 WPM.\\n#### 52'],\n",
       " 'reasonings': [None],\n",
       " 'logprobs': [],\n",
       " 'argmax_logits_eq_gold': [],\n",
       " 'logits': None,\n",
       " 'unconditioned_logprobs': None,\n",
       " 'truncated_tokens_count': 0,\n",
       " 'padded_tokens_count': 0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pipeline.get_details().values())[0][0].model_response.__getstate__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27b7fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lighteval.tasks.registry import Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c900b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed no task name. This should only occur if you are using the CLI to inspect tasks.\n",
      "Careful, the task musr:murder_mysteries is using evaluation data to build the few shot examples.\n",
      "Careful, the task musr:object_placements is using evaluation data to build the few shot examples.\n",
      "Careful, the task musr:team_allocation is using evaluation data to build the few shot examples.\n",
      "Careful, the task tiny:truthfulqa is using evaluation data to build the few shot examples.\n",
      "Careful, the task arithmetic:1dc is using evaluation data to build the few shot examples.\n",
      "Careful, the task arithmetic:2da is using evaluation data to build the few shot examples.\n",
      "Careful, the task arithmetic:2dm is using evaluation data to build the few shot examples.\n",
      "Careful, the task arithmetic:2ds is using evaluation data to build the few shot examples.\n",
      "Careful, the task arithmetic:3da is using evaluation data to build the few shot examples.\n",
      "Careful, the task arithmetic:3ds is using evaluation data to build the few shot examples.\n",
      "Careful, the task arithmetic:4da is using evaluation data to build the few shot examples.\n",
      "Careful, the task arithmetic:4ds is using evaluation data to build the few shot examples.\n",
      "Careful, the task arithmetic:5da is using evaluation data to build the few shot examples.\n",
      "Careful, the task arithmetic:5ds is using evaluation data to build the few shot examples.\n",
      "Careful, the task math_500 is using evaluation data to build the few shot examples.\n",
      "Careful, the task gpqa:mc is using evaluation data to build the few shot examples.\n",
      "Careful, the task gpqa:diamond is using evaluation data to build the few shot examples.\n",
      "Careful, the task gpqa:extended is using evaluation data to build the few shot examples.\n",
      "Careful, the task gpqa:main is using evaluation data to build the few shot examples.\n",
      "Careful, the task prost is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:arxiv is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:bibliotik is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:commoncrawl is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:dm-mathematics is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:enron is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:europarl is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:freelaw is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:github is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:gutenberg is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:hackernews is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:nih-exporter is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:opensubtitles is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:openwebtext2 is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:pubmed-abstracts is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:pubmed-central is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:stackexchange is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:upsto is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:wikipedia is using evaluation data to build the few shot examples.\n",
      "Careful, the task the_pile:youtubesubtitles is using evaluation data to build the few shot examples.\n",
      "Careful, the task storycloze:2016 is using evaluation data to build the few shot examples.\n",
      "Careful, the task storycloze:2018 is using evaluation data to build the few shot examples.\n",
      "Careful, the task real_toxicity_prompts is using evaluation data to build the few shot examples.\n",
      "Careful, the task hle is using evaluation data to build the few shot examples.\n",
      "Careful, the task race:high is using evaluation data to build the few shot examples.\n",
      "Careful, the task xwinograd:en is using evaluation data to build the few shot examples.\n",
      "Careful, the task xwinograd:fr is using evaluation data to build the few shot examples.\n",
      "Careful, the task xwinograd:jp is using evaluation data to build the few shot examples.\n",
      "Careful, the task xwinograd:pt is using evaluation data to build the few shot examples.\n",
      "Careful, the task xwinograd:ru is using evaluation data to build the few shot examples.\n",
      "Careful, the task xwinograd:zh is using evaluation data to build the few shot examples.\n",
      "Careful, the task entity_data_imputation:Restaurant is using evaluation data to build the few shot examples.\n",
      "Careful, the task imdb:contrastset is using evaluation data to build the few shot examples.\n",
      "Careful, the task truthfulqa:gen is using evaluation data to build the few shot examples.\n",
      "Careful, the task truthfulqa:mc is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmmu_pro:standard-4 is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmmu_pro:standard-10 is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmmu_pro:vision is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:adjunct_island is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:anaphor_gender_agreement is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:anaphor_number_agreement is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:animate_subject_passive is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:animate_subject_trans is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:causative is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:complex_NP_island is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:drop_argument is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:ellipsis_n_bar_1 is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:ellipsis_n_bar_2 is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:existential_there_object_raising is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:inchoative is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:intransitive is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:irregular_past_participle_adjectives is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:irregular_past_participle_verbs is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:only_npi_scope is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:passive_1 is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:passive_2 is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:principle_A_c_command is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:principle_A_reconstruction is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:regular_plural_subject_verb_agreement_1 is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:regular_plural_subject_verb_agreement_2 is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:sentential_negation_npi_licensor_present is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:sentential_negation_npi_scope is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:sentential_subject_island is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:superlative_quantifiers_1 is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:superlative_quantifiers_2 is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:tough_vs_raising_1 is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:tough_vs_raising_2 is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:transitive is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:wh_island is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:wh_questions_object_gap is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:wh_questions_subject_gap is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:wh_questions_subject_gap_long_distance is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:wh_vs_that_no_gap is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:wh_vs_that_no_gap_long_distance is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:wh_vs_that_with_gap is using evaluation data to build the few shot examples.\n",
      "Careful, the task blimp:wh_vs_that_with_gap_long_distance is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:aqua-rat is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:gaokao-biology is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:gaokao-chemistry is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:gaokao-chinese is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:gaokao-english is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:gaokao-geography is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:gaokao-history is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:gaokao-mathqa is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:gaokao-physics is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:logiqa-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:logiqa-zh is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:lsat-ar is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:lsat-lr is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:lsat-rc is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:sat-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:sat-en-without-passage is using evaluation data to build the few shot examples.\n",
      "Careful, the task agieval:sat-math is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt14:de-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt16:en-cs is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt19:en-cs is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt19:en-de is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt19:en-fi is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt19:en-gu is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt19:en-kk is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt19:en-lt is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt19:en-ru is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt19:en-zh is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt19:fi-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt19:fr-de is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt19:gu-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt19:kk-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt19:lt-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt19:ru-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt19:zh-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:cs-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:de-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:en-de is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:en-iu is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:en-ja is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:en-km is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:en-pl is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:en-ps is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:en-ru is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:en-ta is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:en-zh is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:fr-de is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:iu-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:ja-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:km-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:pl-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:ps-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:ru-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:ta-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task wmt20:zh-en is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:abstract_algebra is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:anatomy is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:astronomy is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:business_ethics is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:clinical_knowledge is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:college_biology is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:college_chemistry is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:college_computer_science is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:college_mathematics is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:college_medicine is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:college_physics is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:computer_security is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:conceptual_physics is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:econometrics is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:electrical_engineering is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:elementary_mathematics is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:formal_logic is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:global_facts is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:high_school_biology is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:high_school_chemistry is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:high_school_computer_science is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:high_school_european_history is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:high_school_geography is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:high_school_government_and_politics is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:high_school_macroeconomics is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:high_school_mathematics is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:high_school_microeconomics is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:high_school_physics is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:high_school_psychology is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:high_school_statistics is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:high_school_us_history is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:high_school_world_history is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:human_aging is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:human_sexuality is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:international_law is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:jurisprudence is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:logical_fallacies is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:machine_learning is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:management is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:marketing is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:medical_genetics is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:miscellaneous is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:moral_disputes is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:moral_scenarios is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:nutrition is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:philosophy is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:prehistory is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:professional_accounting is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:professional_law is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:professional_medicine is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:professional_psychology is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:public_relations is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:security_studies is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:sociology is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:us_foreign_policy is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:virology is using evaluation data to build the few shot examples.\n",
      "Careful, the task mmlu_redux_2:world_religions is using evaluation data to build the few shot examples.\n",
      "Careful, the task qa4mre:2011 is using evaluation data to build the few shot examples.\n",
      "Careful, the task qa4mre:2012 is using evaluation data to build the few shot examples.\n",
      "Careful, the task qa4mre:2013 is using evaluation data to build the few shot examples.\n",
      "Careful, the task boolq:contrastset is using evaluation data to build the few shot examples.\n",
      "Careful, the task pubmedqa is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:causal_judgment is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:date_understanding is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:disambiguation_qa is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:geometric_shapes is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:logical_deduction_five_objects is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:logical_deduction_seven_objects is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:logical_deduction_three_objects is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:movie_recommendation is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:navigate is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:reasoning_about_colored_objects is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:ruin_names is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:salient_translation_error_detection is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:snarks is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:sports_understanding is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:temporal_sequences is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:tracking_shuffled_objects_five_objects is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:tracking_shuffled_objects_seven_objects is using evaluation data to build the few shot examples.\n",
      "Careful, the task bigbench_hard:tracking_shuffled_objects_three_objects is using evaluation data to build the few shot examples.\n",
      "Careful, the task aa_omniscience is using evaluation data to build the few shot examples.\n",
      "Careful, the task gsm_plus is using evaluation data to build the few shot examples.\n",
      "Careful, the task asdiv is using evaluation data to build the few shot examples.\n",
      "Careful, the task twitterAAE:aa is using evaluation data to build the few shot examples.\n",
      "Careful, the task twitterAAE:white is using evaluation data to build the few shot examples.\n",
      "Careful, the task unscramble:anagrams1 is using evaluation data to build the few shot examples.\n",
      "Careful, the task unscramble:anagrams2 is using evaluation data to build the few shot examples.\n",
      "Careful, the task unscramble:cycle_letters is using evaluation data to build the few shot examples.\n",
      "Careful, the task unscramble:random_insertion is using evaluation data to build the few shot examples.\n",
      "Careful, the task unscramble:reversed_words is using evaluation data to build the few shot examples.\n",
      "Careful, the task aime24 is using evaluation data to build the few shot examples.\n",
      "Careful, the task aime24_gpassk is using evaluation data to build the few shot examples.\n",
      "Careful, the task aime25 is using evaluation data to build the few shot examples.\n",
      "Careful, the task aime25_gpassk is using evaluation data to build the few shot examples.\n",
      "Careful, the task mixeval_easy:multichoice is using evaluation data to build the few shot examples.\n",
      "Careful, the task mixeval_easy:freeform is using evaluation data to build the few shot examples.\n",
      "Careful, the task mixeval_hard:multichoice is using evaluation data to build the few shot examples.\n",
      "Careful, the task mixeval_hard:freeform is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_release_v1 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_release_v2 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_release_v3 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_release_v4 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_release_v5 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_release_v6 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_release_latest is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_v1 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_v2 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_v3 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_v4 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_v5 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_v6 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_v1_v2 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_v1_v3 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_v1_v4 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_v1_v5 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_v2_v3 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_v2_v4 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_v2_v5 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_v3_v4 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration_v3_v5 is using evaluation data to build the few shot examples.\n",
      "Careful, the task lcb:codegeneration is using evaluation data to build the few shot examples.\n",
      "Careful, the task mt_bench is using evaluation data to build the few shot examples.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lexglue:case_hold|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x10c7d6210>,\n",
       " 'lexglue:ecthr_a|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a12c30>,\n",
       " 'lexglue:ecthr_b|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a316d0>,\n",
       " 'lexglue:eurlex|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a31910>,\n",
       " 'lexglue:ledgar|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a317f0>,\n",
       " 'lexglue:scotus|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a31a30>,\n",
       " 'lexglue:unfair_tos|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a31b50>,\n",
       " 'musr:murder_mysteries|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a31c70>,\n",
       " 'musr:object_placements|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a31eb0>,\n",
       " 'musr:team_allocation|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a31fd0>,\n",
       " 'emotion_classification|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a320f0>,\n",
       " 'drop|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a32210>,\n",
       " 'anli:r1|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a32330>,\n",
       " 'anli:r2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a32450>,\n",
       " 'anli:r3|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a32570>,\n",
       " 'tiny:winogrande|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a32690>,\n",
       " 'tiny:arc|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a327b0>,\n",
       " 'tiny:hellaswag|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a328d0>,\n",
       " 'tiny:mmlu|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a329f0>,\n",
       " 'tiny:truthfulqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a32b10>,\n",
       " 'tiny:gsm8k|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a32c30>,\n",
       " 'arithmetic:1dc|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a32d50>,\n",
       " 'arithmetic:2da|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a32e70>,\n",
       " 'arithmetic:2dm|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a32f90>,\n",
       " 'arithmetic:2ds|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a330b0>,\n",
       " 'arithmetic:3da|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a331d0>,\n",
       " 'arithmetic:3ds|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a332f0>,\n",
       " 'arithmetic:4da|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a33410>,\n",
       " 'arithmetic:4ds|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a33530>,\n",
       " 'arithmetic:5da|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a33650>,\n",
       " 'arithmetic:5ds|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a33770>,\n",
       " 'math_500|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a33890>,\n",
       " 'simpleqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a339b0>,\n",
       " 'qasper|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a33ad0>,\n",
       " 'commonsenseqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a33bf0>,\n",
       " 'slr_bench_all|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a33d10>,\n",
       " 'slr_bench_basic|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a33e30>,\n",
       " 'slr_bench_easy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3c050>,\n",
       " 'slr_bench_medium|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3c170>,\n",
       " 'slr_bench_hard|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3c290>,\n",
       " 'gpqa:mc|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3c3b0>,\n",
       " 'gpqa:diamond|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3c4d0>,\n",
       " 'gpqa:extended|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3c5f0>,\n",
       " 'gpqa:main|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3c710>,\n",
       " 'mmlu:abstract_algebra|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3c830>,\n",
       " 'mmlu:anatomy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3c950>,\n",
       " 'mmlu:astronomy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3ca70>,\n",
       " 'mmlu:business_ethics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3cb90>,\n",
       " 'mmlu:clinical_knowledge|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3ccb0>,\n",
       " 'mmlu:college_biology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3cdd0>,\n",
       " 'mmlu:college_chemistry|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3cef0>,\n",
       " 'mmlu:college_computer_science|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3d010>,\n",
       " 'mmlu:college_mathematics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3d130>,\n",
       " 'mmlu:college_medicine|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3d250>,\n",
       " 'mmlu:college_physics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3d370>,\n",
       " 'mmlu:computer_security|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3d490>,\n",
       " 'mmlu:conceptual_physics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3d5b0>,\n",
       " 'mmlu:econometrics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3d6d0>,\n",
       " 'mmlu:electrical_engineering|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3d7f0>,\n",
       " 'mmlu:elementary_mathematics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3d910>,\n",
       " 'mmlu:formal_logic|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3da30>,\n",
       " 'mmlu:global_facts|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3db50>,\n",
       " 'mmlu:high_school_biology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3dc70>,\n",
       " 'mmlu:high_school_chemistry|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3dd90>,\n",
       " 'mmlu:high_school_computer_science|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3deb0>,\n",
       " 'mmlu:high_school_european_history|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3dfd0>,\n",
       " 'mmlu:high_school_geography|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3e0f0>,\n",
       " 'mmlu:high_school_government_and_politics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3e210>,\n",
       " 'mmlu:high_school_macroeconomics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3e330>,\n",
       " 'mmlu:high_school_mathematics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3e450>,\n",
       " 'mmlu:high_school_microeconomics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3e570>,\n",
       " 'mmlu:high_school_physics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3e690>,\n",
       " 'mmlu:high_school_psychology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3e7b0>,\n",
       " 'mmlu:high_school_statistics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3e8d0>,\n",
       " 'mmlu:high_school_us_history|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3e9f0>,\n",
       " 'mmlu:high_school_world_history|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3eb10>,\n",
       " 'mmlu:human_aging|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3ec30>,\n",
       " 'mmlu:human_sexuality|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3ed50>,\n",
       " 'mmlu:international_law|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3ee70>,\n",
       " 'mmlu:jurisprudence|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3ef90>,\n",
       " 'mmlu:logical_fallacies|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3f0b0>,\n",
       " 'mmlu:machine_learning|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3f1d0>,\n",
       " 'mmlu:management|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3f2f0>,\n",
       " 'mmlu:marketing|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3f410>,\n",
       " 'mmlu:medical_genetics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3f530>,\n",
       " 'mmlu:miscellaneous|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3f650>,\n",
       " 'mmlu:moral_disputes|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3f770>,\n",
       " 'mmlu:moral_scenarios|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3f890>,\n",
       " 'mmlu:nutrition|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3f9b0>,\n",
       " 'mmlu:philosophy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3fad0>,\n",
       " 'mmlu:prehistory|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3fbf0>,\n",
       " 'mmlu:professional_accounting|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3fd10>,\n",
       " 'mmlu:professional_law|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3fe30>,\n",
       " 'mmlu:professional_medicine|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a38050>,\n",
       " 'mmlu:professional_psychology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a38170>,\n",
       " 'mmlu:public_relations|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a38290>,\n",
       " 'mmlu:security_studies|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a383b0>,\n",
       " 'mmlu:sociology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a384d0>,\n",
       " 'mmlu:us_foreign_policy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a385f0>,\n",
       " 'mmlu:virology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a38710>,\n",
       " 'mmlu:world_religions|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a38830>,\n",
       " 'prost|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a38950>,\n",
       " 'jeopardy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a38a70>,\n",
       " 'lambada:standard|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a38b90>,\n",
       " 'lambada:standard_cloze|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a38cb0>,\n",
       " 'the_pile:arxiv|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a38dd0>,\n",
       " 'the_pile:bibliotik|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a38ef0>,\n",
       " 'the_pile:commoncrawl|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a39010>,\n",
       " 'the_pile:dm-mathematics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a39130>,\n",
       " 'the_pile:enron|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a39250>,\n",
       " 'the_pile:europarl|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a39370>,\n",
       " 'the_pile:freelaw|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a39490>,\n",
       " 'the_pile:github|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a395b0>,\n",
       " 'the_pile:gutenberg|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a396d0>,\n",
       " 'the_pile:hackernews|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a397f0>,\n",
       " 'the_pile:nih-exporter|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a39910>,\n",
       " 'the_pile:opensubtitles|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a39a30>,\n",
       " 'the_pile:openwebtext2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a39b50>,\n",
       " 'the_pile:pubmed-abstracts|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a39c70>,\n",
       " 'the_pile:pubmed-central|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a39d90>,\n",
       " 'the_pile:stackexchange|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a39eb0>,\n",
       " 'the_pile:upsto|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a39fd0>,\n",
       " 'the_pile:wikipedia|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3a0f0>,\n",
       " 'the_pile:youtubesubtitles|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3a210>,\n",
       " 'swag|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3a330>,\n",
       " 'med_mcqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3a450>,\n",
       " 'med_paragraph_simplification|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3a570>,\n",
       " 'med_qa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3a690>,\n",
       " 'gsm8k|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3a7b0>,\n",
       " 'winogrande|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3a8d0>,\n",
       " 'mathqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3a9f0>,\n",
       " 'xcopa:en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3ab10>,\n",
       " 'xcopa:et|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3ac30>,\n",
       " 'xcopa:ht|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3ad50>,\n",
       " 'xcopa:it|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3ae70>,\n",
       " 'xcopa:id|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3af90>,\n",
       " 'xcopa:qu|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3b0b0>,\n",
       " 'xcopa:sw|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3b1d0>,\n",
       " 'xcopa:zh|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3b2f0>,\n",
       " 'xcopa:ta|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3b410>,\n",
       " 'xcopa:th|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3b530>,\n",
       " 'xcopa:tr|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3b650>,\n",
       " 'xcopa:vi|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3b770>,\n",
       " 'numeracy:linear_example|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3b890>,\n",
       " 'numeracy:linear_standard|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3b9b0>,\n",
       " 'numeracy:parabola_example|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3bad0>,\n",
       " 'numeracy:parabola_standard|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3bbf0>,\n",
       " 'numeracy:paraboloid_example|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3bd10>,\n",
       " 'numeracy:paraboloid_standard|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a3be30>,\n",
       " 'numeracy:plane_example|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a44050>,\n",
       " 'numeracy:plane_standard|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a44170>,\n",
       " 'wikifact:applies_to_jurisdiction|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a44290>,\n",
       " 'wikifact:atomic_number|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a443b0>,\n",
       " 'wikifact:author|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a444d0>,\n",
       " 'wikifact:employer|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a445f0>,\n",
       " 'wikifact:field_of_work|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a44710>,\n",
       " 'wikifact:file_extension|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a44830>,\n",
       " 'wikifact:genetic_association|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a44950>,\n",
       " 'wikifact:instrument|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a44a70>,\n",
       " 'wikifact:language_of_work_or_name|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a44b90>,\n",
       " 'wikifact:languages_spoken_written_or_signed|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a44cb0>,\n",
       " 'wikifact:laws_applied|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a44dd0>,\n",
       " 'wikifact:located_in_the_administrative_territorial_entity|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a44ef0>,\n",
       " 'wikifact:location|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a45010>,\n",
       " 'wikifact:location_of_discovery|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a45130>,\n",
       " 'wikifact:location_of_formation|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a45250>,\n",
       " 'wikifact:member_of|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a45370>,\n",
       " 'wikifact:member_of_political_party|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a45490>,\n",
       " 'wikifact:member_of_sports_team|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a455b0>,\n",
       " 'wikifact:movement|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a456d0>,\n",
       " 'wikifact:headquarters_location|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a457f0>,\n",
       " 'wikifact:industry|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a45910>,\n",
       " 'wikifact:named_after|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a45a30>,\n",
       " 'wikifact:native_language|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a45b50>,\n",
       " 'wikifact:number_of_processor_cores|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a45c70>,\n",
       " 'wikifact:occupation|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a45d90>,\n",
       " 'wikifact:original_language_of_film_or_TV_show|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a45eb0>,\n",
       " 'wikifact:original_network|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a45fd0>,\n",
       " 'wikifact:overrules|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a460f0>,\n",
       " 'wikifact:owned_by|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a46210>,\n",
       " 'wikifact:part_of|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a46330>,\n",
       " 'wikifact:participating_team|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a46450>,\n",
       " 'wikifact:place_of_birth|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a46570>,\n",
       " 'wikifact:place_of_death|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a46690>,\n",
       " 'wikifact:position_played_on_team|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a467b0>,\n",
       " 'wikifact:programming_language|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a468d0>,\n",
       " 'wikifact:recommended_unit_of_measurement|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a469f0>,\n",
       " 'wikifact:record_label|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a46b10>,\n",
       " 'wikifact:religion|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a46c30>,\n",
       " 'wikifact:repealed_by|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a46d50>,\n",
       " 'wikifact:shares_border_with|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a46e70>,\n",
       " 'wikifact:solved_by|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a46f90>,\n",
       " 'wikifact:statement_describes|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a470b0>,\n",
       " 'wikifact:stock_exchange|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a471d0>,\n",
       " 'wikifact:subclass_of|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a472f0>,\n",
       " 'wikifact:subsidiary|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a47410>,\n",
       " 'wikifact:symptoms_and_signs|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a47530>,\n",
       " 'wikifact:therapeutic_area|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a47650>,\n",
       " 'wikifact:time_of_discovery_or_invention|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a47770>,\n",
       " 'wikifact:twinned_administrative_body|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a47890>,\n",
       " 'wikifact:work_location|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a479b0>,\n",
       " 'bbq|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a47ad0>,\n",
       " 'bbq:Age|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a47bf0>,\n",
       " 'bbq:Disability_status|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a47d10>,\n",
       " 'bbq:Gender_identity|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a47e30>,\n",
       " 'bbq:Nationality|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4c050>,\n",
       " 'bbq:Physical_appearance|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4c170>,\n",
       " 'bbq:Race_ethnicity|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4c290>,\n",
       " 'bbq:Race_x_SES|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4c3b0>,\n",
       " 'bbq:Race_x_gender|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4c4d0>,\n",
       " 'bbq:Religion|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4c5f0>,\n",
       " 'bbq:SES|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4c710>,\n",
       " 'bbq:Sexual_orientation|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4c830>,\n",
       " 'med_dialog:healthcaremagic|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4c950>,\n",
       " 'med_dialog:icliniq|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4ca70>,\n",
       " 'storycloze:2016|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4cb90>,\n",
       " 'storycloze:2018|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4ccb0>,\n",
       " 'real_toxicity_prompts|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4cdd0>,\n",
       " 'babi_qa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4cef0>,\n",
       " 'headqa:en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4d010>,\n",
       " 'headqa:es|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4d130>,\n",
       " 'hle|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4d250>,\n",
       " 'race:high|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4d370>,\n",
       " 'olympiad_bench:OE_TO_physics_zh_CEE|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4d490>,\n",
       " 'olympiad_bench:OE_TO_maths_en_COMP|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4d5b0>,\n",
       " 'olympiad_bench:OE_TO_maths_zh_COMP|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4d6d0>,\n",
       " 'olympiad_bench:OE_TO_physics_en_COMP|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4d7f0>,\n",
       " 'olympiad_bench:OE_TO_maths_zh_CEE|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4d910>,\n",
       " 'coqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4da30>,\n",
       " 'civil_comments:LGBTQ|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4db50>,\n",
       " 'civil_comments:black|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4dc70>,\n",
       " 'civil_comments:christian|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4dd90>,\n",
       " 'civil_comments:female|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4deb0>,\n",
       " 'civil_comments:male|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4dfd0>,\n",
       " 'civil_comments:muslim|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4e0f0>,\n",
       " 'civil_comments:other_religions|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4e210>,\n",
       " 'civil_comments:white|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4e330>,\n",
       " 'xwinograd:en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4e450>,\n",
       " 'xwinograd:fr|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4e570>,\n",
       " 'xwinograd:jp|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4e690>,\n",
       " 'xwinograd:pt|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4e7b0>,\n",
       " 'xwinograd:ru|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4e8d0>,\n",
       " 'xwinograd:zh|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4e9f0>,\n",
       " 'entity_data_imputation:Buy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4eb10>,\n",
       " 'entity_data_imputation:Restaurant|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4ec30>,\n",
       " 'xstory_cloze:en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4ed50>,\n",
       " 'xstory_cloze:ru|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4ee70>,\n",
       " 'xstory_cloze:zh|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4ef90>,\n",
       " 'xstory_cloze:es|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4f0b0>,\n",
       " 'xstory_cloze:ar|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4f1d0>,\n",
       " 'xstory_cloze:hi|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4f2f0>,\n",
       " 'xstory_cloze:id|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4f410>,\n",
       " 'xstory_cloze:te|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4f530>,\n",
       " 'xstory_cloze:sw|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4f650>,\n",
       " 'xstory_cloze:eu|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4f770>,\n",
       " 'xstory_cloze:my|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4f890>,\n",
       " 'arc_agi_2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4f9b0>,\n",
       " 'natural_questions|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4fad0>,\n",
       " 'imdb|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4fbf0>,\n",
       " 'imdb:contrastset|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4fd10>,\n",
       " 'glue:cola|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a4fe30>,\n",
       " 'glue:mnli|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a50050>,\n",
       " 'glue:mnli_mismatched|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a50170>,\n",
       " 'glue:mrpc|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a50290>,\n",
       " 'glue:qnli|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a503b0>,\n",
       " 'glue:qqp|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a504d0>,\n",
       " 'glue:rte|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a505f0>,\n",
       " 'glue:sst2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a50710>,\n",
       " 'glue:stsb|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a50830>,\n",
       " 'glue:wnli|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a50950>,\n",
       " 'super_glue:boolq|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a50a70>,\n",
       " 'super_glue:cb|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a50b90>,\n",
       " 'super_glue:copa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a50cb0>,\n",
       " 'super_glue:rte|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a50dd0>,\n",
       " 'super_glue:multirc|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a50ef0>,\n",
       " 'super_glue:wic|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a51010>,\n",
       " 'super_glue:wsc|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a51130>,\n",
       " 'sciq|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a51250>,\n",
       " 'synthetic_reasoning:induction|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a51370>,\n",
       " 'synthetic_reasoning:natural_easy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a51490>,\n",
       " 'synthetic_reasoning:natural_hard|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a515b0>,\n",
       " 'synthetic_reasoning:pattern_match|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a516d0>,\n",
       " 'synthetic_reasoning:variable_substitution|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a517f0>,\n",
       " 'truthfulqa:gen|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a51910>,\n",
       " 'truthfulqa:mc|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a51a30>,\n",
       " 'narrativeqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a51b50>,\n",
       " 'mmmu_pro:standard-4|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a51c70>,\n",
       " 'mmmu_pro:standard-10|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a51d90>,\n",
       " 'mmmu_pro:vision|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a51eb0>,\n",
       " 'hellaswag|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a51fd0>,\n",
       " 'mgsm:en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a520f0>,\n",
       " 'mgsm:es|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a52210>,\n",
       " 'mgsm:fr|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a52330>,\n",
       " 'mgsm:de|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a52450>,\n",
       " 'mgsm:ru|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a52570>,\n",
       " 'mgsm:zh|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a52690>,\n",
       " 'mgsm:ja|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a527b0>,\n",
       " 'mgsm:th|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a528d0>,\n",
       " 'mgsm:sw|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a529f0>,\n",
       " 'mgsm:bn|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a52b10>,\n",
       " 'mgsm:te|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a52c30>,\n",
       " 'lsat_qa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a52d50>,\n",
       " 'lsat_qa:assignment|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a52e70>,\n",
       " 'lsat_qa:grouping|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a52f90>,\n",
       " 'lsat_qa:miscellaneous|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a530b0>,\n",
       " 'lsat_qa:ordering|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a531d0>,\n",
       " 'squad_v2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a532f0>,\n",
       " 'blimp:adjunct_island|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a53410>,\n",
       " 'blimp:anaphor_gender_agreement|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a53530>,\n",
       " 'blimp:anaphor_number_agreement|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a53650>,\n",
       " 'blimp:animate_subject_passive|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a53770>,\n",
       " 'blimp:animate_subject_trans|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a53890>,\n",
       " 'blimp:causative|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a539b0>,\n",
       " 'blimp:complex_NP_island|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a53ad0>,\n",
       " 'blimp:drop_argument|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a53bf0>,\n",
       " 'blimp:ellipsis_n_bar_1|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a53d10>,\n",
       " 'blimp:ellipsis_n_bar_2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a53e30>,\n",
       " 'blimp:existential_there_object_raising|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5c050>,\n",
       " 'blimp:inchoative|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5c170>,\n",
       " 'blimp:intransitive|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5c290>,\n",
       " 'blimp:irregular_past_participle_adjectives|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5c3b0>,\n",
       " 'blimp:irregular_past_participle_verbs|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5c4d0>,\n",
       " 'blimp:only_npi_scope|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5c5f0>,\n",
       " 'blimp:passive_1|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5c710>,\n",
       " 'blimp:passive_2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5c830>,\n",
       " 'blimp:principle_A_c_command|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5c950>,\n",
       " 'blimp:principle_A_reconstruction|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5ca70>,\n",
       " 'blimp:regular_plural_subject_verb_agreement_1|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5cb90>,\n",
       " 'blimp:regular_plural_subject_verb_agreement_2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5ccb0>,\n",
       " 'blimp:sentential_negation_npi_licensor_present|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5cdd0>,\n",
       " 'blimp:sentential_negation_npi_scope|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5cef0>,\n",
       " 'blimp:sentential_subject_island|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5d010>,\n",
       " 'blimp:superlative_quantifiers_1|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5d130>,\n",
       " 'blimp:superlative_quantifiers_2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5d250>,\n",
       " 'blimp:tough_vs_raising_1|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5d370>,\n",
       " 'blimp:tough_vs_raising_2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5d490>,\n",
       " 'blimp:transitive|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5d5b0>,\n",
       " 'blimp:wh_island|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5d6d0>,\n",
       " 'blimp:wh_questions_object_gap|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5d7f0>,\n",
       " 'blimp:wh_questions_subject_gap|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5d910>,\n",
       " 'blimp:wh_questions_subject_gap_long_distance|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5da30>,\n",
       " 'blimp:wh_vs_that_no_gap|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5db50>,\n",
       " 'blimp:wh_vs_that_no_gap_long_distance|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5dc70>,\n",
       " 'blimp:wh_vs_that_with_gap|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5dd90>,\n",
       " 'blimp:wh_vs_that_with_gap_long_distance|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5deb0>,\n",
       " 'dyck_language:2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5dfd0>,\n",
       " 'dyck_language:3|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5e0f0>,\n",
       " 'dyck_language:4|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5e210>,\n",
       " 'mmlu_pro|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5e330>,\n",
       " 'agieval:aqua-rat|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5e450>,\n",
       " 'agieval:gaokao-biology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5e570>,\n",
       " 'agieval:gaokao-chemistry|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5e690>,\n",
       " 'agieval:gaokao-chinese|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5e7b0>,\n",
       " 'agieval:gaokao-english|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5e8d0>,\n",
       " 'agieval:gaokao-geography|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5e9f0>,\n",
       " 'agieval:gaokao-history|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5eb10>,\n",
       " 'agieval:gaokao-mathqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5ec30>,\n",
       " 'agieval:gaokao-physics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5ed50>,\n",
       " 'agieval:logiqa-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5ee70>,\n",
       " 'agieval:logiqa-zh|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5ef90>,\n",
       " 'agieval:lsat-ar|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5f0b0>,\n",
       " 'agieval:lsat-lr|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5f1d0>,\n",
       " 'agieval:lsat-rc|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5f2f0>,\n",
       " 'agieval:sat-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5f410>,\n",
       " 'agieval:sat-en-without-passage|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5f530>,\n",
       " 'agieval:sat-math|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5f650>,\n",
       " 'aimo_progress_prize_1|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5f770>,\n",
       " 'wmt14:de-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5f890>,\n",
       " 'wmt16:en-cs|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5f9b0>,\n",
       " 'wmt19:en-cs|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5fad0>,\n",
       " 'wmt19:en-de|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5fbf0>,\n",
       " 'wmt19:en-fi|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a31d90>,\n",
       " 'wmt19:en-gu|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5fe30>,\n",
       " 'wmt19:en-kk|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a60050>,\n",
       " 'wmt19:en-lt|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a60170>,\n",
       " 'wmt19:en-ru|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a60290>,\n",
       " 'wmt19:en-zh|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a603b0>,\n",
       " 'wmt19:fi-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a604d0>,\n",
       " 'wmt19:fr-de|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a605f0>,\n",
       " 'wmt19:gu-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a60710>,\n",
       " 'wmt19:kk-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a60830>,\n",
       " 'wmt19:lt-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a60950>,\n",
       " 'wmt19:ru-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a60a70>,\n",
       " 'wmt19:zh-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a60b90>,\n",
       " 'wmt20:cs-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a60cb0>,\n",
       " 'wmt20:de-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a60dd0>,\n",
       " 'wmt20:en-de|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a60ef0>,\n",
       " 'wmt20:en-iu|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a61010>,\n",
       " 'wmt20:en-ja|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a61130>,\n",
       " 'wmt20:en-km|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a61250>,\n",
       " 'wmt20:en-pl|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a61370>,\n",
       " 'wmt20:en-ps|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a61490>,\n",
       " 'wmt20:en-ru|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a615b0>,\n",
       " 'wmt20:en-ta|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a616d0>,\n",
       " 'wmt20:en-zh|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a617f0>,\n",
       " 'wmt20:fr-de|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a61910>,\n",
       " 'wmt20:iu-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a61a30>,\n",
       " 'wmt20:ja-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a61b50>,\n",
       " 'wmt20:km-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a61c70>,\n",
       " 'wmt20:pl-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a61d90>,\n",
       " 'wmt20:ps-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a61eb0>,\n",
       " 'wmt20:ru-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a61fd0>,\n",
       " 'wmt20:ta-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a620f0>,\n",
       " 'wmt20:zh-en|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a62210>,\n",
       " 'mmlu_redux_2:abstract_algebra|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a62330>,\n",
       " 'mmlu_redux_2:anatomy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a62450>,\n",
       " 'mmlu_redux_2:astronomy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a62570>,\n",
       " 'mmlu_redux_2:business_ethics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a62690>,\n",
       " 'mmlu_redux_2:clinical_knowledge|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a627b0>,\n",
       " 'mmlu_redux_2:college_biology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a628d0>,\n",
       " 'mmlu_redux_2:college_chemistry|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a629f0>,\n",
       " 'mmlu_redux_2:college_computer_science|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a62b10>,\n",
       " 'mmlu_redux_2:college_mathematics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a62c30>,\n",
       " 'mmlu_redux_2:college_medicine|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a62d50>,\n",
       " 'mmlu_redux_2:college_physics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a62e70>,\n",
       " 'mmlu_redux_2:computer_security|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a62f90>,\n",
       " 'mmlu_redux_2:conceptual_physics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a630b0>,\n",
       " 'mmlu_redux_2:econometrics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a631d0>,\n",
       " 'mmlu_redux_2:electrical_engineering|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a632f0>,\n",
       " 'mmlu_redux_2:elementary_mathematics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a63410>,\n",
       " 'mmlu_redux_2:formal_logic|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a63530>,\n",
       " 'mmlu_redux_2:global_facts|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a63650>,\n",
       " 'mmlu_redux_2:high_school_biology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a63770>,\n",
       " 'mmlu_redux_2:high_school_chemistry|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a63890>,\n",
       " 'mmlu_redux_2:high_school_computer_science|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a639b0>,\n",
       " 'mmlu_redux_2:high_school_european_history|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a63ad0>,\n",
       " 'mmlu_redux_2:high_school_geography|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a63bf0>,\n",
       " 'mmlu_redux_2:high_school_government_and_politics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a63d10>,\n",
       " 'mmlu_redux_2:high_school_macroeconomics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a63e30>,\n",
       " 'mmlu_redux_2:high_school_mathematics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a68050>,\n",
       " 'mmlu_redux_2:high_school_microeconomics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a68170>,\n",
       " 'mmlu_redux_2:high_school_physics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a68290>,\n",
       " 'mmlu_redux_2:high_school_psychology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a683b0>,\n",
       " 'mmlu_redux_2:high_school_statistics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a684d0>,\n",
       " 'mmlu_redux_2:high_school_us_history|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a685f0>,\n",
       " 'mmlu_redux_2:high_school_world_history|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a68710>,\n",
       " 'mmlu_redux_2:human_aging|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a68830>,\n",
       " 'mmlu_redux_2:human_sexuality|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a68950>,\n",
       " 'mmlu_redux_2:international_law|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a68a70>,\n",
       " 'mmlu_redux_2:jurisprudence|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a68b90>,\n",
       " 'mmlu_redux_2:logical_fallacies|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a68cb0>,\n",
       " 'mmlu_redux_2:machine_learning|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a68dd0>,\n",
       " 'mmlu_redux_2:management|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a68ef0>,\n",
       " 'mmlu_redux_2:marketing|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a69010>,\n",
       " 'mmlu_redux_2:medical_genetics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a69130>,\n",
       " 'mmlu_redux_2:miscellaneous|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a69250>,\n",
       " 'mmlu_redux_2:moral_disputes|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a69370>,\n",
       " 'mmlu_redux_2:moral_scenarios|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a69490>,\n",
       " 'mmlu_redux_2:nutrition|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a695b0>,\n",
       " 'mmlu_redux_2:philosophy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a696d0>,\n",
       " 'mmlu_redux_2:prehistory|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a697f0>,\n",
       " 'mmlu_redux_2:professional_accounting|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a69910>,\n",
       " 'mmlu_redux_2:professional_law|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a69a30>,\n",
       " 'mmlu_redux_2:professional_medicine|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a69b50>,\n",
       " 'mmlu_redux_2:professional_psychology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a69c70>,\n",
       " 'mmlu_redux_2:public_relations|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a69d90>,\n",
       " 'mmlu_redux_2:security_studies|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a69eb0>,\n",
       " 'mmlu_redux_2:sociology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a69fd0>,\n",
       " 'mmlu_redux_2:us_foreign_policy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6a0f0>,\n",
       " 'mmlu_redux_2:virology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6a210>,\n",
       " 'mmlu_redux_2:world_religions|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6a330>,\n",
       " 'siqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6a450>,\n",
       " 'covid_dialogue|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6a570>,\n",
       " 'qa4mre:2011|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6a690>,\n",
       " 'qa4mre:2012|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6a7b0>,\n",
       " 'qa4mre:2013|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6a8d0>,\n",
       " 'boolq|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a5fd10>,\n",
       " 'boolq:contrastset|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6a9f0>,\n",
       " 'wikitext:103:document_level|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6ac30>,\n",
       " 'summarization:cnn-dm|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6ad50>,\n",
       " 'summarization:xsum|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6ae70>,\n",
       " 'summarization:xsum-sampled|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6af90>,\n",
       " 'pubmedqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6b0b0>,\n",
       " 'ethics:commonsense|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6b1d0>,\n",
       " 'ethics:deontology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6b2f0>,\n",
       " 'ethics:justice|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6b410>,\n",
       " 'ethics:utilitarianism|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6b530>,\n",
       " 'ethics:virtue|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6b650>,\n",
       " 'bigbench_hard:causal_judgment|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6b770>,\n",
       " 'bigbench_hard:date_understanding|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6b890>,\n",
       " 'bigbench_hard:disambiguation_qa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6b9b0>,\n",
       " 'bigbench_hard:geometric_shapes|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6bad0>,\n",
       " 'bigbench_hard:logical_deduction_five_objects|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6bbf0>,\n",
       " 'bigbench_hard:logical_deduction_seven_objects|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6bd10>,\n",
       " 'bigbench_hard:logical_deduction_three_objects|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6be30>,\n",
       " 'bigbench_hard:movie_recommendation|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7c050>,\n",
       " 'bigbench_hard:navigate|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7c170>,\n",
       " 'bigbench_hard:reasoning_about_colored_objects|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7c290>,\n",
       " 'bigbench_hard:ruin_names|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7c3b0>,\n",
       " 'bigbench_hard:salient_translation_error_detection|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7c4d0>,\n",
       " 'bigbench_hard:snarks|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7c5f0>,\n",
       " 'bigbench_hard:sports_understanding|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7c710>,\n",
       " 'bigbench_hard:temporal_sequences|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7c830>,\n",
       " 'bigbench_hard:tracking_shuffled_objects_five_objects|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7c950>,\n",
       " 'bigbench_hard:tracking_shuffled_objects_seven_objects|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7ca70>,\n",
       " 'bigbench_hard:tracking_shuffled_objects_three_objects|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7cb90>,\n",
       " 'legalsupport|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7ccb0>,\n",
       " 'legal_summarization:billsum|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7cdd0>,\n",
       " 'legal_summarization:eurlexsum|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7cef0>,\n",
       " 'legal_summarization:multilexsum|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7d010>,\n",
       " 'logiqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7d130>,\n",
       " 'math:algebra|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7d250>,\n",
       " 'math:counting_and_probability|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7d370>,\n",
       " 'math:geometry|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7d490>,\n",
       " 'math:intermediate_algebra|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7d5b0>,\n",
       " 'math:number_theory|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7d6d0>,\n",
       " 'math:prealgebra|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7d7f0>,\n",
       " 'math:precalculus|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7d910>,\n",
       " 'openbookqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7da30>,\n",
       " 'bold|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7db50>,\n",
       " 'bold:gender|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7dc70>,\n",
       " 'bold:political_ideology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7dd90>,\n",
       " 'bold:profession|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7deb0>,\n",
       " 'bold:race|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7dfd0>,\n",
       " 'bold:religious_ideology|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7e0f0>,\n",
       " 'bigbench:abstract_narrative_understanding|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7e210>,\n",
       " 'bigbench:anachronisms|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7e330>,\n",
       " 'bigbench:analogical_similarity|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7e450>,\n",
       " 'bigbench:moral_permissibility|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7e570>,\n",
       " 'bigbench:movie_dialog_same_or_different|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7e690>,\n",
       " 'bigbench:movie_recommendation|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7e7b0>,\n",
       " 'bigbench:mult_data_wrangling|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7e8d0>,\n",
       " 'bigbench:simple_ethical_questions|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7e9f0>,\n",
       " 'bigbench:simple_text_editing|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7eb10>,\n",
       " 'bigbench:snarks|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7ec30>,\n",
       " 'bigbench:social_iqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7ed50>,\n",
       " 'bigbench:social_support|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7ee70>,\n",
       " 'bigbench:sports_understanding|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7ef90>,\n",
       " 'bigbench:strange_stories|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7f0b0>,\n",
       " 'bigbench:strategyqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7f1d0>,\n",
       " 'bigbench:sufficient_information|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7f2f0>,\n",
       " 'bigbench:suicide_risk|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7f410>,\n",
       " 'bigbench:swahili_english_proverbs|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7f530>,\n",
       " 'bigbench:swedish_to_german_proverbs|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7f650>,\n",
       " 'bigbench:symbol_interpretation|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7f770>,\n",
       " 'bigbench:tellmewhy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7f890>,\n",
       " 'bigbench:temporal_sequences|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7f9b0>,\n",
       " 'bigbench:tense|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7fad0>,\n",
       " 'bigbench:timedial|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7fbf0>,\n",
       " 'bigbench:topical_chat|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7fd10>,\n",
       " 'bigbench:tracking_shuffled_objects|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a7fe30>,\n",
       " 'bigbench:understanding_fables|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a84050>,\n",
       " 'bigbench:undo_permutation|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a84170>,\n",
       " 'bigbench:unit_conversion|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a84290>,\n",
       " 'bigbench:unit_interpretation|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a843b0>,\n",
       " 'bigbench:unnatural_in_context_learning|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a844d0>,\n",
       " 'bigbench:vitaminc_fact_verification|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a845f0>,\n",
       " 'bigbench:what_is_the_tao|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a84710>,\n",
       " 'bigbench:which_wiki_edit|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a84830>,\n",
       " 'bigbench:winowhy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a84950>,\n",
       " 'bigbench:word_sorting|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a84a70>,\n",
       " 'bigbench:word_unscrambling|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a84b90>,\n",
       " 'arc:challenge|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a84cb0>,\n",
       " 'arc:easy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a84dd0>,\n",
       " 'raft:ade_corpus_v2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a84ef0>,\n",
       " 'raft:banking_77|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a85010>,\n",
       " 'raft:neurips_impact_statement_risks|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a85130>,\n",
       " 'raft:one_stop_english|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a85250>,\n",
       " 'raft:overruling|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a85370>,\n",
       " 'raft:semiconductor_org_types|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a85490>,\n",
       " 'raft:systematic_review_inclusion|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a855b0>,\n",
       " 'raft:tai_safety_research|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a856d0>,\n",
       " 'raft:terms_of_service|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a857f0>,\n",
       " 'raft:tweet_eval_hate|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a85910>,\n",
       " 'raft:twitter_complaints|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a85a30>,\n",
       " 'aa_omniscience|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a85b50>,\n",
       " 'gsm_plus|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a85c70>,\n",
       " 'asdiv|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a85d90>,\n",
       " 'twitterAAE:aa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a85eb0>,\n",
       " 'twitterAAE:white|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a85fd0>,\n",
       " 'lextreme:brazilian_court_decisions_judgment|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a860f0>,\n",
       " 'lextreme:brazilian_court_decisions_unanimity|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a86210>,\n",
       " 'lextreme:covid19_emergency_event|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a86330>,\n",
       " 'lextreme:german_argument_mining|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a86450>,\n",
       " 'lextreme:greek_legal_code_chapter|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a86570>,\n",
       " 'lextreme:greek_legal_code_subject|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a86690>,\n",
       " 'lextreme:greek_legal_code_volume|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a867b0>,\n",
       " 'lextreme:greek_legal_ner|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a868d0>,\n",
       " 'lextreme:legalnero|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a869f0>,\n",
       " 'lextreme:lener_br|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a86b10>,\n",
       " 'lextreme:mapa_coarse|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a86c30>,\n",
       " 'lextreme:mapa_fine|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a86d50>,\n",
       " 'lextreme:multi_eurlex_level_1|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a86e70>,\n",
       " 'lextreme:multi_eurlex_level_2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a86f90>,\n",
       " 'lextreme:multi_eurlex_level_3|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a870b0>,\n",
       " 'lextreme:online_terms_of_service_clause_topics|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a871d0>,\n",
       " 'lextreme:online_terms_of_service_unfairness_levels|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a872f0>,\n",
       " 'lextreme:swiss_judgment_prediction|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a87410>,\n",
       " 'webqs|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a87530>,\n",
       " 'toxigen|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a87650>,\n",
       " 'unscramble:anagrams1|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a87770>,\n",
       " 'unscramble:anagrams2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a87890>,\n",
       " 'unscramble:cycle_letters|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a879b0>,\n",
       " 'unscramble:random_insertion|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a87ad0>,\n",
       " 'unscramble:reversed_words|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a87bf0>,\n",
       " 'entity_matching:Abt_Buy|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a87d10>,\n",
       " 'entity_matching:Amazon_Google|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a87e30>,\n",
       " 'entity_matching:Beer|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a90050>,\n",
       " 'entity_matching:Company|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a90170>,\n",
       " 'entity_matching:DBLP_ACM|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a90290>,\n",
       " 'entity_matching:DBLP_GoogleScholar|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a903b0>,\n",
       " 'entity_matching:Dirty_DBLP_ACM|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a904d0>,\n",
       " 'entity_matching:Dirty_DBLP_GoogleScholar|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a905f0>,\n",
       " 'entity_matching:Dirty_Walmart_Amazon|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a90710>,\n",
       " 'entity_matching:Dirty_iTunes_Amazon|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a90830>,\n",
       " 'entity_matching=Fodors_Zagats|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a90950>,\n",
       " 'entity_matching:Walmart_Amazon|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a90a70>,\n",
       " 'entity_matching:iTunes_Amazon|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a90b90>,\n",
       " 'aime24|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a90cb0>,\n",
       " 'aime24_gpassk|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a90dd0>,\n",
       " 'aime25|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a90ef0>,\n",
       " 'aime25_gpassk|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a91010>,\n",
       " 'quac|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a91130>,\n",
       " 'triviaqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a91250>,\n",
       " 'piqa|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a91370>,\n",
       " 'ifeval|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a91490>,\n",
       " 'mixeval_easy:multichoice|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a915b0>,\n",
       " 'mixeval_easy:freeform|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a916d0>,\n",
       " 'mixeval_hard:multichoice|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a917f0>,\n",
       " 'mixeval_hard:freeform|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a91910>,\n",
       " 'lcb:codegeneration_release_v1|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a91a30>,\n",
       " 'lcb:codegeneration_release_v2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a91b50>,\n",
       " 'lcb:codegeneration_release_v3|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a91c70>,\n",
       " 'lcb:codegeneration_release_v4|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a91d90>,\n",
       " 'lcb:codegeneration_release_v5|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a6ab10>,\n",
       " 'lcb:codegeneration_release_v6|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a91fd0>,\n",
       " 'lcb:codegeneration_release_latest|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a91eb0>,\n",
       " 'lcb:codegeneration_v1|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a920f0>,\n",
       " 'lcb:codegeneration_v2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a92330>,\n",
       " 'lcb:codegeneration_v3|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a92450>,\n",
       " 'lcb:codegeneration_v4|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a92570>,\n",
       " 'lcb:codegeneration_v5|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a92690>,\n",
       " 'lcb:codegeneration_v6|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a92210>,\n",
       " 'lcb:codegeneration_v1_v2|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a928d0>,\n",
       " 'lcb:codegeneration_v1_v3|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a929f0>,\n",
       " 'lcb:codegeneration_v1_v4|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a92b10>,\n",
       " 'lcb:codegeneration_v1_v5|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a92c30>,\n",
       " 'lcb:codegeneration_v2_v3|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a92d50>,\n",
       " 'lcb:codegeneration_v2_v4|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a92e70>,\n",
       " 'lcb:codegeneration_v2_v5|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a92f90>,\n",
       " 'lcb:codegeneration_v3_v4|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a930b0>,\n",
       " 'lcb:codegeneration_v3_v5|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a927b0>,\n",
       " 'lcb:codegeneration|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a932f0>,\n",
       " 'mt_bench|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a93410>,\n",
       " 'ifbench_test|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a93530>,\n",
       " 'ifbench_multiturn|0': <lighteval.tasks.lighteval_task.LightevalTask at 0x145a93650>}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Registry().load_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63af635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
